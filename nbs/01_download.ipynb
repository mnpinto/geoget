{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download\n",
    "\n",
    "> Functions to download remote sensing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rasterio.coords import BoundingBox, disjoint_bounds\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "import re\n",
    "import os\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "from nbdev.imports import test_eq\n",
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import pdb\n",
    "\n",
    "from geoget.external import geturl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.export import notebook2script\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Ladsweb():\n",
    "    def __init__(self, product:str, collection:str, tstart:str, tend:str,\n",
    "                 bbox:list, bands:list=None, coordsOrTiles:str=\"coords\", daynight:str=\"DNB\",\n",
    "                 repName:str='GEO', repPixSize:float=0.01, repResample:str='bilinear',\n",
    "                 doMosaic:str='False', **kwargs):\n",
    "        self.product, self.collection = product, collection\n",
    "        self.tstart, self.tend, self.bbox, self.bands = tstart, tend, bbox, bands\n",
    "        self.coordsOrTiles, self.daynight, self.repName = coordsOrTiles, daynight, repName\n",
    "        self.repPixSize, self.repResample, self.doMosaic = repPixSize, repResample, doMosaic\n",
    "        self._maxOrderSize, self._authFile = 1800, os.path.expanduser('~/.ladsweb')\n",
    "        \n",
    "    @property\n",
    "    def _email(self):\n",
    "        with open(self._authFile, 'r') as f:\n",
    "            data = json.load(f)['email']\n",
    "        assert len(data) > 0\n",
    "        assert '@' in data\n",
    "        return data\n",
    "    \n",
    "    @property\n",
    "    def _key(self):\n",
    "        with open(self._authFile, 'r') as f:\n",
    "            data = json.load(f)['key']\n",
    "        assert len(data) > 0\n",
    "        return data\n",
    "        \n",
    "    def search_files(self):\n",
    "        \"Search for files for the product, region and time span given.\"\n",
    "        url = (f\"https://modwebsrv.modaps.eosdis.nasa.gov/axis2/services/MODAPSservices/\" + \n",
    "            f\"searchForFiles?product={self.product}&collection={self.collection}&\" + \n",
    "            f\"start={self.tstart}&stop={self.tend}&north={self.bbox[3]}&south={self.bbox[1]}\" + \n",
    "            f\"&west={self.bbox[0]}&east={self.bbox[2]}&coordsOrTiles={self.coordsOrTiles}\" +\n",
    "            f\"&dayNightBoth={self.daynight}\")\n",
    "        return re.findall('<return>(.*?)</return>', requests.get(url).text)\n",
    "    \n",
    "    def download_raw_files(self, path_save:Path, replace=False):\n",
    "        authFile = os.path.expanduser('~/.ladsweb')\n",
    "        with open(authFile, 'r') as f:\n",
    "            f = json.load(f)\n",
    "            email = f['email']\n",
    "            auth = f['key']\n",
    "\n",
    "        if isinstance(path_save, str):\n",
    "            path_save = Path(path_save)\n",
    "            path_save.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Get order ids\n",
    "        print('Searching for files...')\n",
    "        order_ids = self.search_files()\n",
    "\n",
    "        # Search filenames\n",
    "        filenames = []\n",
    "        for order_id in progress_bar(order_ids):\n",
    "            url = f'https://ladsweb.modaps.eosdis.nasa.gov/details/file/{self.collection}/{order_id}'\n",
    "            # Ladsweb will sometimes return 504 timeouts, so we retry up to 10 times\n",
    "            good_file = False\n",
    "            for _ in range(10):\n",
    "                if not good_file:\n",
    "                    try:\n",
    "                        file = re.findall('<td>File Name</td><td>(.*?)</td>', requests.get(url).text)[0]\n",
    "                        filenames.append(file)\n",
    "                        good_file = True\n",
    "                    except:\n",
    "                        warnings.warn(f'Unable to get {url} (most likely receieved 504 Gateway Time-out). Retrying.', UserWarning)\n",
    "                        sleep(10)\n",
    "\n",
    "        pattern = r'^\\w+.A(20[0-9][0-9])([0-3][0-9][0-9])..*$'\n",
    "\n",
    "        # Extract time from filenames\n",
    "        times = []\n",
    "        for f in filenames:\n",
    "            x = re.search(pattern, f)\n",
    "            if x is not None:\n",
    "                year, doy = map(x.group, [1,2])\n",
    "            times.append(pd.Timestamp(f'{year}-01-01') + pd.Timedelta(days=int(doy)-1))\n",
    "\n",
    "        # Download Files\n",
    "        print('Downloading files...')\n",
    "        for filename, time in progress_bar(zip(filenames, times), total=len(filenames)):\n",
    "            year = time.year\n",
    "            doy = time.dayofyear\n",
    "            url = f'https://ladsweb.modaps.eosdis.nasa.gov/archive/allData/' \\\n",
    "                  f'{self.collection}/{self.product}/{year}/{doy:03d}/{filename}'\n",
    "            fsave = f'{path_save/filename}'\n",
    "            if not Path(fsave).is_file() or replace:\n",
    "                good_file = False\n",
    "                for _ in range(10):\n",
    "                    if not good_file:\n",
    "                        with open(fsave, mode='w+b') as fh:\n",
    "                            try: \n",
    "                                geturl(f'{url}', auth, fh)\n",
    "                            except Exception as e: \n",
    "                                warnings.warn(f'Unable to get {url}. Exception {e}', UserWarning)\n",
    "                        try: \n",
    "                            Dataset(fsave, mode='r')\n",
    "                            good_file = True\n",
    "                        except: \n",
    "                            warnings.warn(f'Failed to open netcdf. Trying to download again.')\n",
    "                            os.remove(fsave)\n",
    "                            sleep(10)\n",
    "            else: warnings.warn(f'{filename} already exists in {path_save} and replace is set to False')\n",
    "\n",
    "    def order_size(self):\n",
    "        \"Calculates the number of files in the order.\"\n",
    "        if self.bands is None: \n",
    "            raise Exception(\"`bands` list required to calculate order_size.\")\n",
    "        return len(self.search_files())*len(self.bands)\n",
    "    \n",
    "    def split_times(self, maxOrderSize=None):\n",
    "        \"Split a single order into multiple orders if the order size is too large.\"\n",
    "        if maxOrderSize is None: maxOrderSize = self._maxOrderSize\n",
    "        order_size = self.order_size()\n",
    "        if order_size <= maxOrderSize:\n",
    "            return [self]\n",
    "        n_splits = order_size // maxOrderSize + 1\n",
    "        times = pd.date_range(self.tstart, self.tend)\n",
    "        bk = -(len(times) % n_splits)\n",
    "        if bk == 0: bk = None\n",
    "        splits = np.split(times[:bk], n_splits)\n",
    "        splits[-1] = splits[-1].append(times[bk:])\n",
    "        times = [(str(t[0]), str(t[-1]+pd.Timedelta(days=1-1e-5).round('s'))) for t in splits]\n",
    "        tstart, tend = zip(*times)\n",
    "        kwargs = self.__dict__\n",
    "        group = []\n",
    "        for ti, tf in zip(tstart, tend):\n",
    "            kwargs['tstart'], kwargs['tend'] = ti, tf\n",
    "            group.append(Ladsweb(**kwargs))\n",
    "        return group\n",
    "    \n",
    "    def send_order(self, ids):\n",
    "        \"Send order for a set of ids obtained with `search_files` method.\"\n",
    "        ids = ','.join(ids)\n",
    "        bands = ','.join([self.product + f'___{b}' for b in self.bands])\n",
    "        url = (f\"http://modwebsrv.modaps.eosdis.nasa.gov/axis2/services/MODAPSservices/\" +\n",
    "            f\"orderFiles?fileIds={ids}\" + \n",
    "            f\"&subsetDataLayer={bands}\" + \n",
    "            f\"&geoSubsetNorth={self.bbox[3]}\" + \n",
    "            f\"&geoSubsetSouth={self.bbox[1]}\" + \n",
    "            f\"&geoSubsetEast={self.bbox[2]}\" + \n",
    "            f\"&geoSubsetWest={self.bbox[0]}\" + \n",
    "            f\"&reprojectionName={self.repName}\" +\n",
    "            f\"&reprojectionOutputPixelSize={self.repPixSize}\" + \n",
    "            f\"&reprojectionResampleType={self.repResample}\" +\n",
    "            f\"&doMosaic={self.doMosaic}\" + \n",
    "            f\"&email={self._email}\")\n",
    "        return re.findall('<return>(.*?)</return>', requests.get(url).text)[0]\n",
    "    \n",
    "    def run(self, path_save):\n",
    "        \"Send request and update log file.\"\n",
    "        ids = self.search_files()\n",
    "        if len(ids) == 0: \n",
    "            warnings.warn(\"No files found\", UserWarning)\n",
    "            return\n",
    "        if self.bands is None: raise Exception(\"A list of `bands` is not defined\")\n",
    "        orderId = self.send_order(ids)\n",
    "        status = order_status(orderId)\n",
    "        update_log(Path(path_save)/'order_log.json', orderId, status)\n",
    "        print(f'New request sent with orderId {orderId}')\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = ''\n",
    "        for k in self.__dict__:\n",
    "            s += f'{k}: {self.__dict__[k]}, '\n",
    "        return s + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Ladsweb.search_files\" class=\"doc_header\"><code>Ladsweb.search_files</code><a href=\"__main__.py#L28\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Ladsweb.search_files</code>()\n",
       "\n",
       "Search for files for the product, region and time span given."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Ladsweb.search_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing files search\n",
    "lads = Ladsweb(\n",
    "    product='NPP_VMAES_L1', \n",
    "    collection='5000', \n",
    "    tstart='2017-10-27 00:00:00',\n",
    "    tend='2017-10-27 23:59:59',\n",
    "    bbox=[-10,36,0,44], # left bottom right top\n",
    "    daynight='D') # D N DNB\n",
    "\n",
    "test_eq(','.join(lads.search_files()), '2857074643,2857122490,2857117946')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Ladsweb.order_size\" class=\"doc_header\"><code>Ladsweb.order_size</code><a href=\"__main__.py#L80\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Ladsweb.order_size</code>()\n",
       "\n",
       "Calculates the number of files in the order."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Ladsweb.split_times\" class=\"doc_header\"><code>Ladsweb.split_times</code><a href=\"__main__.py#L86\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Ladsweb.split_times</code>(**`maxOrderSize`**=*`None`*)\n",
       "\n",
       "Split a single order into multiple orders if the order size is too large."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Ladsweb.send_order\" class=\"doc_header\"><code>Ladsweb.send_order</code><a href=\"__main__.py#L107\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Ladsweb.send_order</code>(**`ids`**)\n",
       "\n",
       "Send order for a set of ids obtained with `search_files` method."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Ladsweb.run\" class=\"doc_header\"><code>Ladsweb.run</code><a href=\"__main__.py#L125\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Ladsweb.run</code>(**`path_save`**)\n",
       "\n",
       "Send request and update log file."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Ladsweb.order_size)\n",
    "show_doc(Ladsweb.split_times)\n",
    "show_doc(Ladsweb.send_order)\n",
    "show_doc(Ladsweb.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_log(file):\n",
    "    \"Read log file.\"\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def update_log(file, orderId, status):\n",
    "    \"Update log file.\"\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    if file.is_file():\n",
    "        stored_data = read_log(file)\n",
    "        if orderId not in [k for k in stored_data]:\n",
    "            stored_data[orderId] = {'status': '', 'time': ''}\n",
    "        if stored_data[orderId]['status'] != status:\n",
    "            stored_data[orderId]['status'] = status\n",
    "            stored_data[orderId]['time'] = current_time\n",
    "    else: \n",
    "        stored_data = {orderId: {'status': status, 'time': current_time}}\n",
    "        \n",
    "    with open(file, 'w') as f:\n",
    "        json.dump(stored_data, f)\n",
    "                \n",
    "def order_status(orderId):\n",
    "    \"Check order status.\"\n",
    "    url = (f\"http://modwebsrv.modaps.eosdis.nasa.gov/axis2/services/MODAPSservices/\" +\n",
    "            f\"getOrderStatus?orderId={orderId}\")\n",
    "    return re.findall('<return>(.*?)</return>', requests.get(url).text)[0]\n",
    "\n",
    "def download_files(orderId, path_save, auth=None):\n",
    "    \"Download files if the order is Available.\"\n",
    "    if auth is None: raise Exception(\"`auth` code is not defined\")\n",
    "    status = order_status(orderId)\n",
    "    if status != 'Available':\n",
    "        msg = f\"Order is not Available, current status is {status}\"\n",
    "        warnings.warn(msg, UserWarning)\n",
    "        return\n",
    "    url = f'https://ladsweb.modaps.eosdis.nasa.gov/archive/orders/{orderId}'\n",
    "    #files = pd.DataFrame(json.loads(geturl(url + '.json', auth))) # no longer available\n",
    "    checksums = geturl(url + f'/checksums_{orderId}', auth)\n",
    "    hdfs = re.findall('(.*?.hdf)', checksums)\n",
    "    ch = [tuple([k for k in h.split(' ') if k != '']) for h in hdfs]\n",
    "    files = pd.DataFrame(ch, columns=['checksum', 'size', 'name'])\n",
    "    files = files.drop('size', axis=1)\n",
    "    #files = pd.merge(files, check_df, how='left', on='name')\n",
    "    files['verified'] = False\n",
    "\n",
    "    for i in progress_bar(range(len(files))):\n",
    "        file, checksum = files.loc[i, ['name', 'checksum']]\n",
    "        csum = None\n",
    "        if (Path(path_save)/file).is_file():\n",
    "            csum = os.popen(f'cksum {str(path_save)}/{file}').read().split(' ')[0]\n",
    "        if csum is None or checksum != csum:\n",
    "            n_tries = 0\n",
    "            while ~files.loc[i, 'verified'] and n_tries<5:\n",
    "                #print(f'Downloading {file}')\n",
    "                with open(Path(path_save)/f'{file}', mode='w+b') as fh:\n",
    "                    try: geturl(f'{url}/{file}', auth, fh)\n",
    "                    except: warnings.warn(f'Unable to get {url}/{file}', UserWarning)\n",
    "                csum = os.popen(f'cksum {str(path_save)}/{file}').read().split(' ')[0]\n",
    "                if str(checksum) == 'nan': checksum = csum\n",
    "                files.loc[i, 'verified'] = checksum == csum\n",
    "                n_tries += 1\n",
    "        elif checksum == csum: files.loc[i, 'verified'] = True\n",
    "    log_file = f'download_log_{orderId}.csv'\n",
    "    files.to_csv(Path(path_save)/log_file)\n",
    "    not_verified = np.sum(~files.verified)\n",
    "    if not_verified > 0:\n",
    "        msg = f\"Checksum failed for {not_verified} files. Check the {log_file}.\"\n",
    "        warnings.warn(msg, UserWarning)\n",
    "    return not_verified\n",
    "\n",
    "def release_order(orderId, email=None):\n",
    "    \"To release order after download the files.\"\n",
    "    if email is None: raise Exception(\"`email` is not defined\")\n",
    "    url = (f\"http://modwebsrv.modaps.eosdis.nasa.gov/axis2/services/MODAPSservices/\" +\n",
    "    f\"releaseOrder?orderId={orderId}&email={email}\")\n",
    "    status = re.findall('<return>(.*?)</return>', requests.get(url).text)[0]\n",
    "    return status == '1'\n",
    "\n",
    "def order_manager(path_save):\n",
    "    \"Manage active orders in log file. Check the status and download the files for each order\"\n",
    "    authFile = os.path.expanduser('~/.ladsweb')\n",
    "    with open(authFile, 'r') as f:\n",
    "        f = json.load(f)\n",
    "        email = f['email']\n",
    "        auth = f['key']\n",
    "        \n",
    "    log_file = Path(path_save)/'order_log.json'\n",
    "    while True:\n",
    "        data = read_log(log_file)\n",
    "\n",
    "        # Update status\n",
    "        for orderId in data:\n",
    "            if data[orderId]['status'] != 'Complete':\n",
    "                status = order_status(orderId)\n",
    "                update_log(log_file, orderId, status)\n",
    "\n",
    "        # Download if available (wait 10 min)\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        for orderId in data:\n",
    "            if data[orderId]['status'] == 'Available':\n",
    "                d = (pd.Timestamp(' '.join(current_time.split('_'))) \n",
    "                     - pd.Timestamp(' '.join(data[orderId]['time'].split('_')))).seconds\n",
    "                if d//60 > 10: \n",
    "                    status = download_files(orderId, path_save, auth)\n",
    "                    if status == 0: \n",
    "                        result = release_order(orderId, email)\n",
    "                        status = 'Complete' if result else 'One or more files not verified'\n",
    "                        update_log(log_file, orderId, status)\n",
    "                        print(f'Files for order {orderId} saved at {path_save}.')\n",
    "        \n",
    "        # Check if stop\n",
    "        n = 0\n",
    "        for orderId in data:\n",
    "            if data[orderId]['status'] in ['Complete', 'One or more files not verified',\n",
    "                                           'Canceled', 'Removed']:\n",
    "                n += 1\n",
    "        stop = len(data) == n\n",
    "        if stop: return\n",
    "        sleep(20)\n",
    "        \n",
    "def run_all(request_list, path_save):\n",
    "    \"Send a list of requests and initiate order manager.\"\n",
    "    for request in request_list:\n",
    "        request.run(path_save)\n",
    "        sleep(5)\n",
    "    order_manager(path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_external.ipynb.\n",
      "Converted 01_download.ipynb.\n",
      "Converted 02_cli.ipynb.\n",
      "Converted 03_era5.ipynb.\n",
      "Converted 04_geo.ipynb.\n",
      "Converted 05_gfs.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f5ab71dd15d736abd678ab0ccef3424f823fce76da7ef90994a1c4d0c13b94c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
